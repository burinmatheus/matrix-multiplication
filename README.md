# ğŸš€ MultiplicaÃ§Ã£o de Matrizes GIGANTES

ImplementaÃ§Ã£o **altamente otimizada** de multiplicaÃ§Ã£o de matrizes usando **MPI** (Message Passing Interface) e **OpenMP** para processamento paralelo hÃ­brido de alto desempenho.

## âš¡ OtimizaÃ§Ãµes AvanÃ§adas

Esta implementaÃ§Ã£o inclui tÃ©cnicas state-of-the-art para maximizar performance:

- **Transposta de Matriz B** â†’ Localidade de cache 10-50x melhor
- **VetorizaÃ§Ã£o SIMD** â†’ 4-8x mais rÃ¡pido no loop interno
- **Escalonamento DinÃ¢mico** â†’ Balanceamento de carga otimizado
- **CompilaÃ§Ã£o Nativa** â†’ InstruÃ§Ãµes especÃ­ficas do CPU (`-march=native`)
- **ParalelizaÃ§Ã£o HÃ­brida** â†’ MPI entre nÃ³s + OpenMP dentro de cada nÃ³

## ğŸ¯ Quick Start

### ğŸš€ Devcontainer (Recomendado)

Ao abrir este projeto no devcontainer:
1. **CompilaÃ§Ã£o automÃ¡tica** do cÃ³digo otimizado
2. **Benchmark automÃ¡tico** executado (18-20 testes, ~2-3 min)
   - Matrizes: 1500Ã—1500 e 800Ã—800
   - ConfiguraÃ§Ãµes: 1, 2, 4, 8 processos MPI Ã— 1, 2, 4, 8 threads OpenMP
   - Identifica o **ponto de quebra** onde paralelismo deixa de compensar
3. **GrÃ¡ficos gerados** automaticamente em `matrix_giant_analysis.png` (9 painÃ©is)
4. **MÃ©tricas salvas** em `matrix_giant_metrics.csv`

ğŸ“Š Os resultados do Ãºltimo benchmark estÃ£o documentados na seÃ§Ã£o "Benchmark AutomÃ¡tico" abaixo.

### 1. Compilar

```bash
make
```

### 2. Testar

```bash
# Teste rÃ¡pido (2 configuraÃ§Ãµes)
make test

# Benchmark automÃ¡tico - 18-20 testes (recomendado)
make bench  # ou: make auto

# ExecuÃ§Ã£o manual
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4

# Matriz GIGANTE 5000Ã—5000
mpirun -np 8 ./bin/matrix_giant -n 5000 -t 8
```

### 3. Benchmark

```bash
# Benchmark automÃ¡tico (18-20 testes com 2 tamanhos de matriz)
make bench

# Apenas regenerar grÃ¡ficos (usa CSV existente)
make plot
```

**O benchmark irÃ¡:**
1. Testar matrizes 1500Ã—1500 e 800Ã—800
2. Variar processos MPI (1, 2, 4, 8) e threads OpenMP (1, 2, 4, 8)
3. Salvar dados em `matrix_giant_metrics.csv`
4. Gerar 9 grÃ¡ficos em `matrix_giant_analysis.png`

## ğŸ“Š Exemplos de Uso

### Exemplo 1: Teste BÃ¡sico
```bash
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4
```

**SaÃ­da:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    MULTIPLICAÃ‡ÃƒO DE MATRIZES GIGANTES - MPI + OpenMP        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Tamanho da matriz      : 2000 x 2000
Elementos totais       : 4.00 milhÃµes
MemÃ³ria por matriz     : 30.52 MB
Processos MPI          : 4
Threads OpenMP/processo: 4
Cores totais           : 16

âœ“ Tempo paralelo: 8.23 segundos
Performance: 1.95 GFLOPS
```

### Exemplo 2: Escalabilidade
```bash
# 1 processo, 4 threads
mpirun -np 1 ./bin/matrix_giant -n 2000 -t 4

# 2 processos, 4 threads cada
mpirun -np 2 ./bin/matrix_giant -n 2000 -t 4

# 4 processos, 4 threads cada
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4

# 8 processos, 4 threads cada
mpirun -np 8 ./bin/matrix_giant -n 2000 -t 4
```

### Exemplo 3: Matriz GIGANTE
```bash
# 5000Ã—5000 = 25 milhÃµes de elementos
mpirun -np 8 ./bin/matrix_giant -n 5000 -t 8
```

### Exemplo 4: Com VerificaÃ§Ã£o
```bash
# Adiciona verificaÃ§Ã£o por amostragem
mpirun -np 2 ./bin/matrix_giant -n 1000 -t 2 --seq --verify
```

## ğŸ® Comandos Make

```bash
make            # Compila o executÃ¡vel
make test       # Teste rÃ¡pido com 2 configuraÃ§Ãµes
make bench      # Benchmark automÃ¡tico (18-20 testes) â­
make auto       # Alias para 'make bench'
make plot       # Regenera apenas os grÃ¡ficos
make examples   # Executa exemplos de uso
make scale      # Teste de escalabilidade
make clean      # Remove binÃ¡rios e resultados
make help       # Mostra ajuda completa
```

## ğŸ“ˆ Performance Esperada

### Tamanhos de Matriz

| Tamanho | Elementos | MemÃ³ria | Tempo (4PÃ—4T)* | GFLOPS |
|---------|-----------|---------|----------------|--------|
| 1000Ã—1000 | 1M | 23 MB | ~1s | ~2.0 |
| 1500Ã—1500 | 2.25M | 52 MB | ~4s | ~1.7 |
| 2000Ã—2000 | 4M | 92 MB | ~8s | ~2.0 |
| 3000Ã—3000 | 9M | 206 MB | ~30s | ~1.8 |
| 5000Ã—5000 | 25M | 572 MB | ~2min | ~2.1 |
| 10000Ã—10000 | 100M | 2.3 GB | ~20min | ~2.0 |

*4 processos MPI Ã— 4 threads OpenMP = 16 cores

### Speedup TÃ­pico

- **8-12x** vs sequencial (16 cores)
- **EficiÃªncia:** 50-75%
- **GFLOPS:** 1.5-2.5 dependendo do hardware

## âš™ï¸ OpÃ§Ãµes do Programa

```bash
mpirun -np <P> ./bin/matrix_giant [opÃ§Ãµes]

OpÃ§Ãµes:
  -n <tamanho>    Tamanho da matriz NxN (padrÃ£o: 2000)
  -t <threads>    Threads OpenMP por processo (padrÃ£o: 4)
  --seq           Executar versÃ£o sequencial (para comparaÃ§Ã£o)
  --verify        Verificar resultado com amostragem
```

## ğŸ”¬ TÃ©cnicas de OtimizaÃ§Ã£o

### 1. Transposta de Matriz B

**Problema:** Acesso com stride causa cache misses
```c
// Antes (ruim para cache)
C[i][j] += A[i][k] * B[k][j];  // B acessado por coluna
```

**SoluÃ§Ã£o:** Transpor B antes da multiplicaÃ§Ã£o
```c
// Depois (Ã³timo para cache)
B_T[j][k] = B[k][j];  // Transpor uma vez
C[i][j] += A[i][k] * B_T[j][k];  // Acesso sequencial
```

**Ganho:** 10-50x em localidade de cache

### 2. VetorizaÃ§Ã£o SIMD

```c
#pragma omp simd reduction(+:sum)
for (int k = 0; k < n; k++) {
    sum += A[i*n + k] * B_T[j*n + k];
}
```

**Ganho:** 4-8x com instruÃ§Ãµes AVX/AVX2

### 3. Escalonamento DinÃ¢mico

```c
#pragma omp parallel for schedule(guided, 8)
```

**Ganho:** Melhor balanceamento em cargas heterogÃªneas

## ğŸ—ï¸ Arquitetura

### ParalelizaÃ§Ã£o HÃ­brida

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Sistema   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
 â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”
 â”‚ P0  â”‚   â”‚ P1  â”‚   â”‚ P2  â”‚  â† Processos MPI
 â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜    (memÃ³ria distribuÃ­da)
    â”‚          â”‚          â”‚
 â”Œâ”€â”€â”¼â”€â”€â”   â”Œâ”€â”€â”¼â”€â”€â”   â”Œâ”€â”€â”¼â”€â”€â”
 T0 T1 T2  T0 T1 T2  T0 T1 T2  â† Threads OpenMP
                                 (memÃ³ria compartilhada)
```

### DistribuiÃ§Ã£o de Dados

- **Matriz A:** Dividida por linhas entre processos MPI
- **Matriz B:** Replicada em todos os processos (broadcast)
- **Matriz C:** Cada processo calcula suas linhas, depois reÃºne no mestre

## ğŸ“Š MÃ©tricas Coletadas

Resultados salvos em `matrix_giant_metrics.csv`:

```csv
MatrixSize,NumProcesses,NumThreads,SeqTime(s),ParTime(s),Speedup,Efficiency(%),GFLOPS
2000,4,4,0.000000,8.234567,0.0000,0.00,1.95
3000,4,4,0.000000,27.891234,0.0000,0.00,1.93
5000,8,8,0.000000,120.456789,0.0000,0.00,2.07
```

### GrÃ¡ficos Gerados

O comando `make bench` ou `make plot` gera automaticamente `matrix_giant_analysis.png` com 9 visualizaÃ§Ãµes:

1. **Performance vs Cores** - GFLOPS por configuraÃ§Ã£o
2. **Tempo vs Tamanho** - Escalabilidade por tamanho de matriz
3. **Performance por Processos MPI** - AnÃ¡lise MPI
4. **Performance por Threads OpenMP** - AnÃ¡lise OpenMP
5. **Heatmap GFLOPS** - Mapa de calor (Processos Ã— Threads)
6. **Escalabilidade** - ComparaÃ§Ã£o com ideal
7. **DistribuiÃ§Ã£o de Performance** - Boxplot por tamanho
8. **Melhor ConfiguraÃ§Ã£o** - Top performances
9. **Resumo EstatÃ­stico** - Tabela com mÃ©tricas principais

## ğŸ› Troubleshooting

### MemÃ³ria Insuficiente

```bash
# Verificar memÃ³ria disponÃ­vel
free -h

# Reduzir tamanho ou processos
mpirun -np 2 ./bin/matrix_giant -n 1500 -t 2
```

### Performance Baixa

```bash
# Verificar uso de CPU
htop

# Otimizar afinidade
export OMP_PROC_BIND=true
export OMP_PLACES=cores
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4
```

### Erros de CompilaÃ§Ã£o

```bash
# Recompilar do zero
make clean
make
```

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ matrix_mult_giant.c       # ImplementaÃ§Ã£o otimizada MPI+OpenMP
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ matrix_giant              # ExecutÃ¡vel
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ auto_benchmark.sh         # Benchmark automÃ¡tico
â”‚   â””â”€â”€ plot_giant.py             # GeraÃ§Ã£o de grÃ¡ficos
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ Dockerfile                # Container com todas dependÃªncias
â”‚   â””â”€â”€ devcontainer.json         # Config do autostart
â”œâ”€â”€ Makefile                      # Build system
â”œâ”€â”€ README.md                     # ğŸ“š DocumentaÃ§Ã£o completa (este arquivo)
â””â”€â”€ requirements.txt              # DependÃªncias Python
```

## ğŸ“ Conceitos

### MPI (Message Passing Interface)
- Paralelismo de **memÃ³ria distribuÃ­da**
- Processos independentes comunicam via mensagens
- Ideal para clusters e sistemas multi-nÃ³

### OpenMP (Open Multi-Processing)
- Paralelismo de **memÃ³ria compartilhada**
- Threads dentro de um processo
- Ideal para sistemas multi-core

### Por que HÃ­brido?
- **MPI:** Entre nÃ³s de um cluster
- **OpenMP:** Dentro de cada nÃ³
- **Resultado:** Uso mÃ¡ximo de recursos disponÃ­veis

## ğŸ“Š Benchmark AutomÃ¡tico (Devcontainer)

Ao abrir o projeto no devcontainer, um benchmark automÃ¡tico Ã© executado identificando o **ponto de quebra** onde paralelismo deixa de valer a pena:

### ğŸ”¬ Resultados do Ãšltimo Benchmark

#### Matriz 1500Ã—1500 (GRANDE)
| Config | Cores | GFLOPS | Tempo | ObservaÃ§Ã£o |
|--------|-------|--------|-------|------------|
| 1PÃ—1T | 1 | 2.00 | 3.37s | Baseline |
| 1PÃ—8T | 8 | 2.06 | 3.28s | âš ï¸ **Overhead!** |
| **2PÃ—1T** | **2** | **3.85** | **1.75s** | âœ… **Melhor** |
| 2PÃ—2T | 4 | 3.19 | 2.11s | +60% vs 1P |
| 4PÃ—2T | 8 | 3.41 | 1.98s | Boa escala |

**ConclusÃ£o:** Matriz grande escala bem atÃ© 2-4 cores. 8 threads em 1 processo = ineficiente!

#### Matriz 800Ã—800 (MÃ‰DIA)
| Config | Cores | GFLOPS | Tempo | ObservaÃ§Ã£o |
|--------|-------|--------|-------|------------|
| 1PÃ—1T | 1 | 2.73 | 0.38s | Baseline |
| 1PÃ—4T | 4 | 2.67 | 0.38s | âš ï¸ Sem ganho |
| 2PÃ—4T | 8 | 4.73 | 0.22s | +73% |
| **4PÃ—2T** | **8** | **6.23** | **0.16s** | ğŸ† **CampeÃ£o** |

**ConclusÃ£o:** Matriz mÃ©dia escala muito bem atÃ© 8 cores com hÃ­brido MPI+OpenMP.

### ğŸ“ LiÃ§Ãµes Aprendidas

1. **OpenMP sozinho (1PÃ—NT) Ã© ineficiente** - sempre use hÃ­brido MPI+OpenMP
2. **Matriz 1500**: ponto de quebra em 8 cores (overhead > ganho)
3. **Matriz 800**: escala bem atÃ© 8 cores
4. **ConfiguraÃ§Ãµes ideais:**
   - Matrizes grandes (â‰¥1500): `mpirun -np 2 ./bin/matrix_giant -n <SIZE> -t 2`
   - Matrizes mÃ©dias (â‰¤1000): `mpirun -np 4 ./bin/matrix_giant -n <SIZE> -t 2`

ğŸ“„ Os resultados ficam em `matrix_giant_metrics.csv` e `matrix_giant_analysis.png` (gerados automaticamente).

## ğŸ“ˆ Limites de Tamanho e MemÃ³ria

### Tamanhos Suportados

| Tamanho | Elementos | MemÃ³ria (3 matrizes) | Tempo Estimado* |
|---------|-----------|---------------------|-----------------|
| 1000Ã—1000 | 1M | 23 MB | ~1s |
| 2000Ã—2000 | 4M | 92 MB | ~8s |
| 3000Ã—3000 | 9M | 206 MB | ~30s |
| 5000Ã—5000 | 25M | 572 MB | ~2 min |
| 8000Ã—8000 | 64M | 1.5 GB | ~10 min |
| 10000Ã—10000 | 100M | 2.3 GB | ~20 min |

*Com 4 processos Ã— 4 threads (16 cores)

### Requisitos de MemÃ³ria por Processo MPI

Com MPI, cada processo precisa:
- Linhas de A: `(NÂ² / P) Ã— 8 bytes`
- Matriz B completa: `NÂ² Ã— 8 bytes` (broadcast)
- Transposta B local: `NÂ² Ã— 8 bytes`
- Resultado local: `(NÂ² / P) Ã— 8 bytes`

**Total por processo â‰ˆ `2NÂ² + 2(NÂ²/P)` bytes**

### CÃ¡lculo de GFLOPS

```
OperaÃ§Ãµes = 2 Ã— NÂ³
GFLOPS = OperaÃ§Ãµes / (Tempo Ã— 10â¹)

Exemplo para 2000Ã—2000:
- OperaÃ§Ãµes = 2 Ã— 2000Â³ = 16 bilhÃµes
- Se tempo = 8s â†’ GFLOPS = 2.0
```

## ğŸ“ TÃ©cnicas AvanÃ§adas

### Cache Blocking (Tiling)
Para matrizes muito grandes, bloquear acesso ao cache:

```c
const int TILE_SIZE = 64;  // Ajustar para L1 cache

for (int ii = 0; ii < n; ii += TILE_SIZE) {
    for (int jj = 0; jj < n; jj += TILE_SIZE) {
        // Multiplicar tile por tile
    }
}
```

### Prefetching Manual
```c
__builtin_prefetch(&A[(i+1)*n], 0, 3);
```

### Algoritmos Alternativos

Para matrizes ainda maiores:

1. **Strassen**: O(n^2.807) vs O(nÂ³)
2. **BLAS (OpenBLAS/MKL)**: OtimizaÃ§Ãµes assembly
3. **ScaLAPACK**: DistribuiÃ§Ã£o 2D de dados
4. **Cannon's Algorithm**: Melhor comunicaÃ§Ã£o MPI
5. **GPU (CUDA/cuBLAS)**: 100-1000x mais rÃ¡pido

## ğŸš€ Roadmap

PossÃ­veis melhorias futuras:

1. âœ… ImplementaÃ§Ã£o bÃ¡sica MPI + OpenMP
2. âœ… OtimizaÃ§Ãµes de cache (transposta)
3. âœ… VetorizaÃ§Ã£o SIMD
4. âœ… Benchmark automÃ¡tico com anÃ¡lise de ponto de quebra
5. âœ… VisualizaÃ§Ã£o com 9 grÃ¡ficos de anÃ¡lise
6. ğŸ”„ Algoritmo de Strassen (O(n^2.807))
7. ğŸ”„ Cache blocking (tiling) otimizado
8. ğŸ”„ Suporte GPU (CUDA/cuBLAS)
9. ğŸ”„ DistribuiÃ§Ã£o 2D de dados
10. ğŸ”„ I/O otimizado para matrizes em disco

## ğŸ“š ReferÃªncias

- [Matrix Multiplication Optimization (Goto)](https://www.cs.utexas.edu/~pingali/CS378/2008sp/papers/gotoPaper.pdf)
- [MPI Matrix Multiplication](https://www.mcs.anl.gov/research/projects/mpi/)
- [OpenMP SIMD Directives](https://www.openmp.org/spec-html/5.0/openmpsu42.html)
- [Cache-Oblivious Algorithms](https://en.wikipedia.org/wiki/Cache-oblivious_algorithm)

## ğŸ¤ Contribuindo

Melhorias sÃ£o bem-vindas! Ãreas de interesse:

- OtimizaÃ§Ãµes adicionais
- Suporte para GPU
- Algoritmos alternativos (Strassen, Coppersmith-Winograd)
- Melhorias no benchmark
- DocumentaÃ§Ã£o

## ğŸ“ LicenÃ§a

MIT License

## ğŸ‘¥ CrÃ©ditos

Projeto educacional demonstrando tÃ©cnicas de computaÃ§Ã£o paralela de alto desempenho.

---

## ğŸš€ ComeÃ§ar Agora

```bash
# Compilar
make

# Teste rÃ¡pido (2 configuraÃ§Ãµes)
make test

# Benchmark automÃ¡tico completo (recomendado)
make bench

# Ver resultados
ls -lh matrix_giant_*.csv matrix_giant_*.png
```

**Dica:** Ao abrir no devcontainer, tudo Ã© executado automaticamente! ğŸ‰
