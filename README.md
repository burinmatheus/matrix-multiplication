# ğŸš€ MultiplicaÃ§Ã£o de Matrizes GIGANTES

ImplementaÃ§Ã£o **altamente otimizada** de multiplicaÃ§Ã£o de matrizes usando **MPI** (Message Passing Interface) e **OpenMP** para processamento paralelo hÃ­brido de alto desempenho.

## âš¡ OtimizaÃ§Ãµes AvanÃ§adas

Esta implementaÃ§Ã£o inclui tÃ©cnicas state-of-the-art para maximizar performance:

- **Transposta de Matriz B** â†’ Localidade de cache 10-50x melhor
- **VetorizaÃ§Ã£o SIMD (Single Instruction, Multiple Data)** â†’ 4-8x mais rÃ¡pido no loop interno (Modelo de processamento paralelo onde uma Ãºnica instruÃ§Ã£o Ã© executada em mÃºltiplos dados simultaneamente.)
- **Escalonamento DinÃ¢mico** â†’ Balanceamento de carga otimizado
- **CompilaÃ§Ã£o Nativa** â†’ InstruÃ§Ãµes especÃ­ficas do CPU (`-march=native`)
- **ParalelizaÃ§Ã£o HÃ­brida** â†’ MPI entre nÃ³s + OpenMP dentro de cada nÃ³

## ğŸ¯ Quick Start

### ğŸš€ Devcontainer (Recomendado)

Ao abrir este projeto no devcontainer:
1. **CompilaÃ§Ã£o automÃ¡tica** do cÃ³digo otimizado
2. **Benchmark automÃ¡tico**
   - Matrizes: 1500Ã—1500 e 800Ã—800
   - ConfiguraÃ§Ãµes: 1, 2, 4, 8 processos MPI Ã— 1, 2, 4, 8 threads OpenMP
3. **GrÃ¡ficos gerados** automaticamente em `matrix_giant_analysis.png`
4. **MÃ©tricas salvas** em `matrix_giant_metrics.csv`

ğŸ“Š Os resultados do Ãºltimo benchmark estÃ£o documentados na seÃ§Ã£o "Benchmark AutomÃ¡tico" abaixo.

### 1. Compilar

```bash
make
```

### 2. Testar

```bash
# Teste rÃ¡pido (2 configuraÃ§Ãµes)
make test

# Benchmark automÃ¡tico - 18-20 testes (recomendado)
make bench  # ou: make auto

# ExecuÃ§Ã£o manual
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4

# Matriz GIGANTE 5000Ã—5000
mpirun -np 8 ./bin/matrix_giant -n 5000 -t 8
```

### 3. Benchmark

```bash
# Benchmark automÃ¡tico (18-20 testes com 2 tamanhos de matriz)
make bench

# Apenas regenerar grÃ¡ficos (usa CSV existente)
make plot
```

**O benchmark irÃ¡:**
1. Testar matrizes 1500Ã—1500 e 800Ã—800
2. Variar processos MPI (1, 2, 4, 8) e threads OpenMP (1, 2, 4, 8)
3. Salvar dados em `matrix_giant_metrics.csv`
4. Gerar 9 grÃ¡ficos em `matrix_giant_analysis.png`

## ğŸ“Š Exemplos de Uso

### Exemplo 1: Teste BÃ¡sico
```bash
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4
```

**SaÃ­da:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    MULTIPLICAÃ‡ÃƒO DE MATRIZES GIGANTES - MPI + OpenMP        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Tamanho da matriz      : 2000 x 2000
Elementos totais       : 4.00 milhÃµes
MemÃ³ria por matriz     : 30.52 MB
Processos MPI          : 4
Threads OpenMP/processo: 4
Cores totais           : 16

âœ“ Tempo paralelo: 8.23 segundos
Performance: 1.95 GFLOPS
```

### Exemplo 2: Escalabilidade
```bash
# 1 processo, 4 threads
mpirun -np 1 ./bin/matrix_giant -n 2000 -t 4

# 2 processos, 4 threads cada
mpirun -np 2 ./bin/matrix_giant -n 2000 -t 4

# 4 processos, 4 threads cada
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4

# 8 processos, 4 threads cada
mpirun -np 8 ./bin/matrix_giant -n 2000 -t 4
```

### Exemplo 3: Matriz GIGANTE
```bash
# 5000Ã—5000 = 25 milhÃµes de elementos
mpirun -np 8 ./bin/matrix_giant -n 5000 -t 8
```

### Exemplo 4: Com VerificaÃ§Ã£o
```bash
# Adiciona verificaÃ§Ã£o por amostragem
mpirun -np 2 ./bin/matrix_giant -n 1000 -t 2 --seq --verify
```

## ğŸ® Comandos Make

```bash
make            # Compila o executÃ¡vel
make test       # Teste rÃ¡pido com 2 configuraÃ§Ãµes
make bench      # Benchmark automÃ¡tico (18-20 testes) â­
make auto       # Alias para 'make bench'
make plot       # Regenera apenas os grÃ¡ficos
make examples   # Executa exemplos de uso
make scale      # Teste de escalabilidade
make clean      # Remove binÃ¡rios e resultados
make help       # Mostra ajuda completa
```

## ğŸ“ˆ Performance Esperada

### Tamanhos de Matriz

| Tamanho | Elementos | MemÃ³ria | Tempo (4PÃ—4T)* | GFLOPS |
|---------|-----------|---------|----------------|--------|
| 1000Ã—1000 | 1M | 23 MB | ~1s | ~2.0 |
| 1500Ã—1500 | 2.25M | 52 MB | ~4s | ~1.7 |
| 2000Ã—2000 | 4M | 92 MB | ~8s | ~2.0 |
| 3000Ã—3000 | 9M | 206 MB | ~30s | ~1.8 |
| 5000Ã—5000 | 25M | 572 MB | ~2min | ~2.1 |
| 10000Ã—10000 | 100M | 2.3 GB | ~20min | ~2.0 |

*4 processos MPI Ã— 4 threads OpenMP = 16 cores

### Speedup TÃ­pico

- **8-12x** vs sequencial (16 cores)
- **EficiÃªncia:** 50-75%
- **GFLOPS:** 1.5-2.5 dependendo do hardware

## âš™ï¸ OpÃ§Ãµes do Programa

```bash
mpirun -np <P> ./bin/matrix_giant [opÃ§Ãµes]

OpÃ§Ãµes:
  -n <tamanho>    Tamanho da matriz NxN (padrÃ£o: 2000)
  -t <threads>    Threads OpenMP por processo (padrÃ£o: 4)
  --seq           Executar versÃ£o sequencial (para comparaÃ§Ã£o)
  --verify        Verificar resultado com amostragem
```

## ğŸ”¬ TÃ©cnicas de OtimizaÃ§Ã£o

### 1. Transposta de Matriz B

**Problema:** Acesso com stride causa cache misses
```c
// Antes (ruim para cache)
C[i][j] += A[i][k] * B[k][j];  // B acessado por coluna
```

**SoluÃ§Ã£o:** Transpor B antes da multiplicaÃ§Ã£o
```c
// Depois (Ã³timo para cache)
B_T[j][k] = B[k][j];  // Transpor uma vez
C[i][j] += A[i][k] * B_T[j][k];  // Acesso sequencial
```

**Ganho:** 10-50x em localidade de cache

### 2. VetorizaÃ§Ã£o SIMD

```c
#pragma omp simd reduction(+:sum)
for (int k = 0; k < n; k++) {
    sum += A[i*n + k] * B_T[j*n + k];
}
```

**Ganho:** 4-8x com instruÃ§Ãµes AVX/AVX2

### 3. Escalonamento DinÃ¢mico

```c
#pragma omp parallel for schedule(guided, 8)
```

**Ganho:** Melhor balanceamento em cargas heterogÃªneas

## ğŸ—ï¸ Arquitetura

### ParalelizaÃ§Ã£o HÃ­brida

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Sistema   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
 â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”
 â”‚ P0  â”‚   â”‚ P1  â”‚   â”‚ P2  â”‚  â† Processos MPI
 â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜    (memÃ³ria distribuÃ­da)
    â”‚          â”‚          â”‚
 â”Œâ”€â”€â”¼â”€â”€â”   â”Œâ”€â”€â”¼â”€â”€â”   â”Œâ”€â”€â”¼â”€â”€â”
 T0 T1 T2  T0 T1 T2  T0 T1 T2  â† Threads OpenMP
                                 (memÃ³ria compartilhada)
```

### DistribuiÃ§Ã£o de Dados

- **Matriz A:** Dividida por linhas entre processos MPI
- **Matriz B:** Replicada em todos os processos (broadcast)
- **Matriz C:** Cada processo calcula suas linhas, depois reÃºne no mestre

## ğŸ“Š MÃ©tricas Coletadas

Resultados salvos em `matrix_giant_metrics.csv`:

```csv
MatrixSize,NumProcesses,NumThreads,SeqTime(s),ParTime(s),Speedup,Efficiency(%),GFLOPS
2000,4,4,0.000000,8.234567,0.0000,0.00,1.95
3000,4,4,0.000000,27.891234,0.0000,0.00,1.93
5000,8,8,0.000000,120.456789,0.0000,0.00,2.07
```

### GrÃ¡ficos Gerados

O comando `make bench` ou `make plot` gera automaticamente `matrix_giant_analysis.png` com 9 visualizaÃ§Ãµes:

1. **Performance vs Cores** - GFLOPS por configuraÃ§Ã£o
2. **Tempo vs Tamanho** - Escalabilidade por tamanho de matriz
3. **Performance por Processos MPI** - AnÃ¡lise MPI
4. **Performance por Threads OpenMP** - AnÃ¡lise OpenMP
5. **Heatmap GFLOPS** - Mapa de calor (Processos Ã— Threads)
6. **Escalabilidade** - ComparaÃ§Ã£o com ideal
7. **DistribuiÃ§Ã£o de Performance** - Boxplot por tamanho
8. **Melhor ConfiguraÃ§Ã£o** - Top performances
9. **Resumo EstatÃ­stico** - Tabela com mÃ©tricas principais

## ğŸ› Troubleshooting

### MemÃ³ria Insuficiente

```bash
# Verificar memÃ³ria disponÃ­vel
free -h

# Reduzir tamanho ou processos
mpirun -np 2 ./bin/matrix_giant -n 1500 -t 2
```

### Performance Baixa

```bash
# Verificar uso de CPU
htop

# Otimizar afinidade
export OMP_PROC_BIND=true
export OMP_PLACES=cores
mpirun -np 4 ./bin/matrix_giant -n 2000 -t 4
```

### Erros de CompilaÃ§Ã£o

```bash
# Recompilar do zero
make clean
make
```

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ matrix_mult_giant.c       # ImplementaÃ§Ã£o otimizada MPI+OpenMP
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ matrix_giant              # ExecutÃ¡vel
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ auto_benchmark.sh         # Benchmark automÃ¡tico
â”‚   â””â”€â”€ plot_giant.py             # GeraÃ§Ã£o de grÃ¡ficos
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ Dockerfile                # Container com todas dependÃªncias
â”‚   â””â”€â”€ devcontainer.json         # Config do autostart
â”œâ”€â”€ Makefile                      # Build system
â”œâ”€â”€ README.md                     # ğŸ“š DocumentaÃ§Ã£o completa (este arquivo)
â””â”€â”€ requirements.txt              # DependÃªncias Python
```

## ğŸ“ Conceitos

### MPI (Message Passing Interface)
- Paralelismo de **memÃ³ria distribuÃ­da**
- Processos independentes comunicam via mensagens
- Ideal para clusters e sistemas multi-nÃ³

### OpenMP (Open Multi-Processing)
- Paralelismo de **memÃ³ria compartilhada**
- Threads dentro de um processo
- Ideal para sistemas multi-core

### Por que HÃ­brido?
- **MPI:** Entre nÃ³s de um cluster
- **OpenMP:** Dentro de cada nÃ³
- **Resultado:** Uso mÃ¡ximo de recursos disponÃ­veis

## ğŸ“Š Benchmark AutomÃ¡tico (Devcontainer)

Ao abrir o projeto no devcontainer, um benchmark automÃ¡tico Ã© executado identificando o **ponto de quebra** onde paralelismo deixa de valer a pena:

### CÃ¡lculo de GFLOPS

```
OperaÃ§Ãµes = 2 Ã— NÂ³
GFLOPS = OperaÃ§Ãµes / (Tempo Ã— 10â¹)

Exemplo para 2000Ã—2000:
- OperaÃ§Ãµes = 2 Ã— 2000Â³ = 16 bilhÃµes
- Se tempo = 8s â†’ GFLOPS = 2.0
```

## ğŸ“ TÃ©cnicas AvanÃ§adas

### Cache Blocking (Tiling)
Para matrizes muito grandes, bloquear acesso ao cache:

```c
const int TILE_SIZE = 64;  // Ajustar para L1 cache

for (int ii = 0; ii < n; ii += TILE_SIZE) {
    for (int jj = 0; jj < n; jj += TILE_SIZE) {
        // Multiplicar tile por tile
    }
}
```

## ğŸ“ LicenÃ§a

MIT License

## ğŸ‘¥ CrÃ©ditos

Projeto educacional demonstrando tÃ©cnicas de computaÃ§Ã£o paralela de alto desempenho.

---

## ğŸš€ ComeÃ§ar Agora

**Dica:** Ao abrir no devcontainer, tudo Ã© executado automaticamente! ğŸ‰ 
